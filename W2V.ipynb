{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the large English NLP model\n",
    "nlp  = spacy.load(\"en_core_web_lg\")\n",
    "# List of words to get vectors for\n",
    "words = [\"dog\", \"cat\", \"tiger\", \"car\", \"bus\", \"bicycle\"]\n",
    "vecs = []\n",
    "# Iterate over each word, get its vector, and print it\n",
    "for word in words:\n",
    "    token = nlp(word)\n",
    "    vecs.append(token.vector)\n",
    "    #print(f\"Word: {word}\\nVector: {token.vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9774cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#use cosine similarity to find similarity between word vectors\n",
    "\n",
    "print(\"Cosine Similarity between word vectors:\")\n",
    "print(f\"Similarity between {words[0]} and {words[1]} : {cosine_similarity([vecs[0]], [vecs[1]])[0][0]}\")\n",
    "print(f\"Similarity between {words[0]} and {words[2]} : {cosine_similarity([vecs[0]], [vecs[2]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[4]} : {cosine_similarity([vecs[3]], [vecs[4]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[5]} : {cosine_similarity([vecs[3]], [vecs[5]])[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class w2v(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(w2v, self).__init__()\n",
    "        self.embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim), requires_grad=True)\n",
    "        self.classifier = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, ids):\n",
    "        words = self.embeddings[ids]\n",
    "        classifier_input = torch.sum(words, dim=0)\n",
    "        output = self.classifier(classifier_input)\n",
    "        return output\n",
    "    # Method to get embeddings for a given id\n",
    "    def getEmbeddings(self, id):\n",
    "        id = torch.LongTensor([id])\n",
    "        return self.embeddings[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"the dog is nice\", \"the cat is lovely\", \"the tiger is wild\",\n",
    "          \"I drive a car\", \"I ride a bus\", \"I like to cycle on a bicycle\"]\n",
    "\n",
    "# Function to create a word to ID mapping\n",
    "def toId(texts):\n",
    "    idDict = {}\n",
    "    nrWords = 0\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in idDict:\n",
    "                idDict[word] = nrWords\n",
    "                nrWords += 1\n",
    "    return idDict\n",
    "\n",
    "idDict = toId(corpus)\n",
    "print(\"Vocabulary:\", idDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070521c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform corpus into sequences of IDs\n",
    "def applyAndTransform(idDict, corpus):\n",
    "    corpusAsIds = []\n",
    "    for text in corpus:\n",
    "        textAsNr = []\n",
    "        for word in text.split():\n",
    "            textAsNr.append(idDict[word])\n",
    "        corpusAsIds.append(textAsNr)\n",
    "    return corpusAsIds\n",
    "\n",
    "\n",
    "applyAndTransform(idDict, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb82b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.LongTensor([[0,2],[1,3],[0,2],[4,3]])\n",
    "targets = torch.LongTensor([[1],[2],[4],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v(vocab_size=len(idDict), embedding_dim=8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    lossAbs = 0\n",
    "    # Iterate over each input-target pair in the dataset\n",
    "    for sample,target in zip(inputs, targets):\n",
    "        output = model(sample) # Forward pass\n",
    "        loss = criterion(output.unsqueeze(0), target) # Compute loss\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "        lossAbs += float(loss) # Accumulate loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {lossAbs}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
