{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "981a081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the large English NLP model\n",
    "nlp  = spacy.load(\"en_core_web_lg\")\n",
    "# List of words to get vectors for\n",
    "words = [\"dog\", \"cat\", \"tiger\", \"car\", \"bus\", \"bicycle\"]\n",
    "vecs = []\n",
    "# Iterate over each word, get its vector, and print it\n",
    "for word in words:\n",
    "    token = nlp(word)\n",
    "    vecs.append(token.vector)\n",
    "    #print(f\"Word: {word}\\nVector: {token.vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9774cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between word vectors:\n",
      "Similarity between dog and cat : 0.8016855120658875\n",
      "Similarity between dog and tiger : 0.43654653429985046\n",
      "Similarity between car and bus : 0.4816959500312805\n",
      "Similarity between car and bicycle : 0.4701661765575409\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#use cosine similarity to find similarity between word vectors\n",
    "\n",
    "print(\"Cosine Similarity between word vectors:\")\n",
    "print(f\"Similarity between {words[0]} and {words[1]} : {cosine_similarity([vecs[0]], [vecs[1]])[0][0]}\")\n",
    "print(f\"Similarity between {words[0]} and {words[2]} : {cosine_similarity([vecs[0]], [vecs[2]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[4]} : {cosine_similarity([vecs[3]], [vecs[4]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[5]} : {cosine_similarity([vecs[3]], [vecs[5]])[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class w2v(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(w2v, self).__init__()\n",
    "        self.embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim), requires_grad=True)\n",
    "        self.classifier = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        words = self.embeddings[ids]\n",
    "        classfier_input = torch.sum(words, dim=0)\n",
    "        output = self.classifier(classfier_input)\n",
    "        return output\n",
    "    \n",
    "    def getEmbeddings(self,id):\n",
    "        id = torch.LongTensor([id])\n",
    "        return self.embeddings[id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
