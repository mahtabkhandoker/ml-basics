{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981a081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the large English NLP model\n",
    "nlp  = spacy.load(\"en_core_web_lg\")\n",
    "# List of words to get vectors for\n",
    "words = [\"dog\", \"cat\", \"tiger\", \"car\", \"bus\", \"bicycle\"]\n",
    "vecs = []\n",
    "# Iterate over each word, get its vector, and print it\n",
    "for word in words:\n",
    "    token = nlp(word)\n",
    "    vecs.append(token.vector)\n",
    "    #print(f\"Word: {word}\\nVector: {token.vector}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9774cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between word vectors:\n",
      "Similarity between dog and cat : 0.8016855120658875\n",
      "Similarity between dog and tiger : 0.43654653429985046\n",
      "Similarity between car and bus : 0.4816959500312805\n",
      "Similarity between car and bicycle : 0.4701661765575409\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#use cosine similarity to find similarity between word vectors\n",
    "\n",
    "print(\"Cosine Similarity between word vectors:\")\n",
    "print(f\"Similarity between {words[0]} and {words[1]} : {cosine_similarity([vecs[0]], [vecs[1]])[0][0]}\")\n",
    "print(f\"Similarity between {words[0]} and {words[2]} : {cosine_similarity([vecs[0]], [vecs[2]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[4]} : {cosine_similarity([vecs[3]], [vecs[4]])[0][0]}\")\n",
    "print(f\"Similarity between {words[3]} and {words[5]} : {cosine_similarity([vecs[3]], [vecs[5]])[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414c7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class w2v(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(w2v, self).__init__()\n",
    "        self.embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim), requires_grad=True)\n",
    "        self.classifier = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # Define the forward pass of the model\n",
    "    def forward(self, ids):\n",
    "        words = self.embeddings[ids]\n",
    "        classifier_input = torch.sum(words, dim=0)\n",
    "        output = self.classifier(classifier_input)\n",
    "        return output\n",
    "    # Method to get embeddings for a given id\n",
    "    def getEmbeddings(self, id):\n",
    "        id = torch.LongTensor([id])\n",
    "        return self.embeddings[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5468a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the': 0, 'dog': 1, 'is': 2, 'nice': 3, 'cat': 4, 'lovely': 5, 'tiger': 6, 'wild': 7, 'I': 8, 'drive': 9, 'a': 10, 'car': 11, 'ride': 12, 'bus': 13, 'like': 14, 'to': 15, 'cycle': 16, 'on': 17, 'bicycle': 18}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"the dog is nice\", \"the cat is lovely\", \"the tiger is wild\",\n",
    "          \"I drive a car\", \"I ride a bus\", \"I like to cycle on a bicycle\"]\n",
    "\n",
    "# Function to create a word to ID mapping\n",
    "def toId(texts):\n",
    "    idDict = {}\n",
    "    nrWords = 0\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in idDict:\n",
    "                idDict[word] = nrWords\n",
    "                nrWords += 1\n",
    "    return idDict\n",
    "\n",
    "idDict = toId(corpus)\n",
    "print(\"Vocabulary:\", idDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070521c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [0, 4, 2, 5],\n",
       " [0, 6, 2, 7],\n",
       " [8, 9, 10, 11],\n",
       " [8, 12, 10, 13],\n",
       " [8, 14, 15, 16, 17, 10, 18]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to transform corpus into sequences of IDs\n",
    "def applyAndTransform(idDict, corpus):\n",
    "    corpusAsIds = []\n",
    "    for text in corpus:\n",
    "        textAsNr = []\n",
    "        for word in text.split():\n",
    "            textAsNr.append(idDict[word])\n",
    "        corpusAsIds.append(textAsNr)\n",
    "    return corpusAsIds\n",
    "\n",
    "\n",
    "applyAndTransform(idDict, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb82b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.LongTensor([[0,2],[1,3],[0,2],[4,3]])\n",
    "targets = torch.LongTensor([[1],[2],[4],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 12.107277631759644\n",
      "Epoch 10, Loss: 2.7728941440582275\n",
      "Epoch 20, Loss: 1.6476396322250366\n",
      "Epoch 30, Loss: 1.5311566554009914\n",
      "Epoch 40, Loss: 1.4864282691851258\n",
      "Epoch 50, Loss: 1.4643561854027212\n",
      "Epoch 60, Loss: 1.45124854426831\n",
      "Epoch 70, Loss: 1.4427716797217727\n",
      "Epoch 80, Loss: 1.4369176852051169\n",
      "Epoch 90, Loss: 1.43267659121193\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = w2v(vocab_size=len(idDict), embedding_dim=8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "targets = targets.view(-1)  # (N,)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    lossAbs = 0\n",
    "    # Iterate over each input-target pair in the dataset\n",
    "    for sample,target in zip(inputs, targets):\n",
    "        output = model(sample) # Forward pass\n",
    "        loss = criterion(output.unsqueeze(0), target) # Compute loss\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "        lossAbs += float(loss) # Accumulate loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {lossAbs}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
